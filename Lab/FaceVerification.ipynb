{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7134,"status":"ok","timestamp":1653983351396,"user":{"displayName":"Nguyen Tram","userId":"06608696287955929010"},"user_tz":-420},"id":"6ZZG7QP7j11Z","outputId":"d3ea935a-9faa-44f7-de3e-57fdf7d51731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mtcnn in /usr/local/lib/python3.7/dist-packages (0.1.1)\n","Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.8.0)\n","Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/rcmalli/keras-vggface.git\n","  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-148pqclq\n","  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-148pqclq\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.21.6)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (3.1.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (7.1.2)\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (2.8.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (3.13)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-vggface==0.6) (1.5.2)\n"]}],"source":["!pip install mtcnn\n","!pip install git+https://github.com/rcmalli/keras-vggface.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5860,"status":"ok","timestamp":1653983380664,"user":{"displayName":"Nguyen Tram","userId":"06608696287955929010"},"user_tz":-420},"id":"2R5VBvg6juEK","outputId":"6c43b17a-1589-4342-82ae-ca8f17cbfed7"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"]}],"source":["%tensorflow_version 1.x\n","# example of face detection with mtcnn\n","import cv2\n","from matplotlib import pyplot\n","from mtcnn.mtcnn import MTCNN\n","\n","# create the detector, using default weights\n","detector = MTCNN()\n","\n","def face_extract(filename, required_size=(224, 224)):\n","  # load image from file\n","  pixels = pyplot.imread(filename)  \n","  \n","  # detect faces in the image\n","  results = detector.detect_faces(pixels)\n","  # extract the bounding box from the first face\n","  faces = []\n","  for item in results:\n","    x1, y1, width, height = item['box']\n","    x2, y2 = x1 + width, y1 + height\n","    # extract the face\n","    face = pixels[y1:y2, x1:x2]\n","    face = cv2.resize(face, required_size )\n","    faces.append(face)\n","  return faces\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":2762,"status":"error","timestamp":1653983452458,"user":{"displayName":"Nguyen Tram","userId":"06608696287955929010"},"user_tz":-420},"id":"PMO7AgUDlvb-","outputId":"ceeb5a61-60d7-4159-fed3-e7445e4ef092"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-f5b411483dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the photo and extract the face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mduong1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Duong1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mduong2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Duong2.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mduong3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Duong3.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvuong1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Vuong1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-64ffb51470f0>\u001b[0m in \u001b[0;36mface_extract\u001b[0;34m(filename, required_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# detect faces in the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;31m# extract the bounding box from the first face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mtcnn/mtcnn.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# We pipe here each of the stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtotal_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mtcnn/mtcnn.py\u001b[0m in \u001b[0;36m__stage1\u001b[0;34m(self, image, scales, stage_status)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mimg_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mout0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[0;32m--> 716\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m    717\u001b[0m     return predict_loop(\n\u001b[1;32m    718\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2469\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2471\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2473\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    570\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    573\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (None, None, 3) but got array with shape (131, 131, 4)"]}],"source":["# load the photo and extract the face\n","duong1 = face_extract('/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Duong1.jpg')[0]\n","duong2 = face_extract('/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Duong2.jpg')[0]\n","duong3 = face_extract('/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Duong3.jpg')[0]\n","vuong1 = face_extract('/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Vuong1.jpg')[0]\n","vuong2 = face_extract('/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Vuong2.jpg')[0]\n","duc1   = face_extract('/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Duc1.jpg')[0]\n","duc2   = face_extract('/content/drive/MyDrive/HK2 - SDH/1.DL-colab/img/Duc2.jpg')[0]\n","faces = [duong1,duong2,duong3,vuong1,vuong2,duc1,duc2]\n","for face in faces:\n","\n","  # plot the extracted face\n","  pyplot.imshow(face)\n","  # show the plot\n","  pyplot.show()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"elapsed":10812,"status":"ok","timestamp":1586836399827,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"},"user_tz":-420},"id":"xtZZmJJ0yMKA","outputId":"cbd22da7-4ea8-4833-a689-14e9371ebf9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n","Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_senet50.h5\n","104947712/104944616 [==============================] - 3s 0us/step\n","Inputs: [<tf.Tensor 'input_4:0' shape=(?, 224, 224, 3) dtype=float32>]\n","Outputs: [<tf.Tensor 'global_average_pooling2d_17/Mean:0' shape=(?, 2048) dtype=float32>]\n"]}],"source":["from keras_vggface.vggface import VGGFace\n","from keras_vggface.utils import preprocess_input\n","from keras_vggface.utils import decode_predictions\n","\n","# create a vggface2 model\n","model = VGGFace(model='senet50',include_top=False, input_shape=(224, 224, 3), pooling='max')\n","# summarize input and output shape\n","print('Inputs: %s' % model.inputs)\n","print('Outputs: %s' % model.outputs)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUiBIb88FmgI","executionInfo":{"status":"ok","timestamp":1653983339116,"user_tz":-420,"elapsed":27751,"user":{"displayName":"Nguyen Tram","userId":"06608696287955929010"}},"outputId":"c391bb7f-e9f2-4e99-f3e6-24060b0a42d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":3528,"status":"ok","timestamp":1586836503043,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"},"user_tz":-420},"id":"jfH6KMK-yUhl","outputId":"42d2a82f-cdb0-4030-b923-1d6e2d43ef1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(7, 2048)\n"]}],"source":["import numpy as np\n","faces = np.array(faces,dtype=\"float64\")\n","faces = preprocess_input(faces,version=2)\n","# perform prediction\n","yhat = model.predict(faces)\n","# convert prediction into names\n","print (yhat.shape)\n","[duong1,duong2,duong3,vuong1,vuong2,duc1,duc2] = yhat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43UxUZVt5z1g"},"outputs":[],"source":["from scipy.spatial.distance import cosine,euclidean\n","def is_match(known_embedding, candidate_embedding, thresh=0.2):\n","\t# calculate distance between embeddings\n","\tscore = cosine(known_embedding, candidate_embedding)\n","\tif score <= thresh:\n","\t\tprint('>face is a Match (%.3f <= %.3f)' % (score, thresh))\n","\telse:\n","\t\tprint('>face is NOT a Match (%.3f > %.3f)' % (score, thresh))\n","  \n","def is_match_euclidean(known_embedding, candidate_embedding, thresh=110):\n","\t# calculate distance between embeddings\n","\tscore = euclidean(known_embedding, candidate_embedding)\n","\tif score <= thresh:\n","\t\tprint('>face is a Match (%.3f <= %.3f)' % (score, thresh))\n","\telse:\n","\t\tprint('>face is NOT a Match (%.3f > %.3f)' % (score, thresh))\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"elapsed":1033,"status":"ok","timestamp":1586836635669,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"},"user_tz":-420},"id":"t0wWswpv69XP","outputId":"e7e362ec-c494-4635-87c0-40221a8382b4"},"outputs":[{"name":"stdout","output_type":"stream","text":[">face is NOT a Match (0.295 > 0.200)\n",">face is NOT a Match (0.449 > 0.200)\n",">face is NOT a Match (0.515 > 0.200)\n"]}],"source":["is_match(duong1,vuong1)\n","is_match(duong1,duc1)\n","is_match(vuong1,duc1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"elapsed":845,"status":"ok","timestamp":1586836711640,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"},"user_tz":-420},"id":"p3hwnOYI_s6Z","outputId":"df69fe01-8732-41f5-83ac-17e3be7bc67d"},"outputs":[{"name":"stdout","output_type":"stream","text":[">face is a Match (0.123 <= 0.200)\n",">face is a Match (0.077 <= 0.200)\n",">face is a Match (0.105 <= 0.200)\n"]}],"source":["is_match(duong1,duong3)\n","is_match(duc1,duc2)\n","is_match(vuong1,vuong2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1586836763038,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"},"user_tz":-420},"id":"VFvXgLr6Cqn2","outputId":"5f400802-cb03-46fc-cef4-5f401a0cbb21"},"outputs":[{"name":"stdout","output_type":"stream","text":[">face is a Match (80.546 <= 110.000)\n",">face is a Match (58.598 <= 110.000)\n",">face is a Match (75.191 <= 110.000)\n"]}],"source":["is_match_euclidean(duong1,duong3)\n","is_match_euclidean(duc1,duc2)\n","is_match_euclidean(vuong1,vuong2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"elapsed":1179,"status":"ok","timestamp":1586836764403,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"},"user_tz":-420},"id":"hSIzGETmCy4E","outputId":"9a8af1dd-6691-4ce8-9059-5764ecc3be23"},"outputs":[{"name":"stdout","output_type":"stream","text":[">face is NOT a Match (126.250 > 110.000)\n",">face is NOT a Match (146.801 > 110.000)\n",">face is NOT a Match (161.294 > 110.000)\n"]}],"source":["is_match_euclidean(duong1,vuong1)\n","is_match_euclidean(duong1,duc1)\n","is_match_euclidean(vuong1,duc1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":886,"status":"ok","timestamp":1586679208543,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"},"user_tz":-420},"id":"_GFE_pZscqhG","outputId":"907f1434-de5e-4fcf-cee1-1860bbc04b2c"},"outputs":[{"name":"stdout","output_type":"stream","text":[">face is NOT a Match (0.224 > 0.200)\n"]}],"source":["is_match(duong1,duong2)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"FaceVerification.ipynb","provenance":[{"file_id":"19D3x_PahjH-qe63smq2_kEL1VzCl0Izp","timestamp":1653981261367},{"file_id":"1kUB6lTLpzEj4297qSiQqeBBjbu2zGFpg","timestamp":1586316839159},{"file_id":"17_DPXce4yiNs-uUyzZZI7_N-sTn3jPrD","timestamp":1586315022278}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}